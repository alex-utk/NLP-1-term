{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 - 10 баллов\n",
    "\n",
    "- Загрузить набор данных по выбору с помощью библиотеки Corus - **1 балл**\n",
    "- Провести релевантную предобработку выбранного датасета: - **6 баллов**\n",
    "  - Нормализация\n",
    "  - Токенизация\n",
    "  - Удаление стоп-слов\n",
    "  - Лемматизация/стемминг\n",
    "\n",
    "- Обеспечена воспроизводимость решения: зафиксированы random_state, ноутбук воспроизводится от начала до конца без ошибок - **2 балла**\n",
    "\n",
    "- Соблюден code style на уровне pep8 и [On writing clean Jupyter notebooks](https://ploomber.io/blog/clean-nbs/) - **1 балл**\n",
    "\n",
    "Инструменты для решения задач предобработки – NLTK, Gensim, Natasha, pymystem, pymorphy2…\n",
    "\n",
    "Для сдачи ДЗ - приложите ссылку на PR (Pull Request) из ветки hw_1 в ветку main в вашем приватном репозитории на github.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from corus import load_rudrec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from hw1_utils import tokenize_and_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скачивание всего необходимого\n",
    "!wget https://github.com/cimm-kzn/RuDReC/raw/master/data/rudrec_annotated.json\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка датасета\n",
    "path = './rudrec_annotated.json'\n",
    "corpus = load_rudrec(path)\n",
    "texts = [record.text for record in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нормирование регуляркой, оставляем только кирилицу и несколько символов пунктуации\n",
    "# в данном датасете для анализа отзывов на лекарства нам вряд ли нужны цифры\n",
    "\n",
    "allowed_symbols = re.compile(u'[0-9]|[^\\u0400-\\u04FF \\-]')\n",
    "ru_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "normed_texts = [allowed_symbols.sub(' ', text).lower()\n",
    "                for text\n",
    "                in texts]\n",
    "# токенизация, лемматизация и постпроцессинг\n",
    "tokenised = [tokenize_and_filter(text, ru_stopwords)\n",
    "             for text\n",
    "             in normed_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спарсим из интереса названия лекарств с сайта аптеки и посмотрим, на что чаще всего отзывы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего наименований: 6770\n"
     ]
    }
   ],
   "source": [
    "url = \"https://apteki.medsi.ru/products/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "all_drugs_items = soup.find_all('li', 'drug-list__item')\n",
    "all_drugs_names = [item.find('a', 'drug-list__link').text.lower()\n",
    "                   for item\n",
    "                   in all_drugs_items]\n",
    "\n",
    "print(f'Всего наименований: {len(all_drugs_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем сколько раз в датасете попадалось то или иное название\n",
    "drug_stats = dict.fromkeys(all_drugs_names, 0)\n",
    "\n",
    "for token_list in tokenised:\n",
    "    for token in token_list:\n",
    "        if token in all_drugs_names:\n",
    "            drug_stats[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего упоминалось: 115\n"
     ]
    }
   ],
   "source": [
    "# отфильтруем нули\n",
    "non_zero_stats = [(name, count)\n",
    "                  for name, count\n",
    "                  in drug_stats.items()\n",
    "                  if count != 0]\n",
    "\n",
    "sorted_by_number = sorted(non_zero_stats, key=lambda tup: tup[1], reverse=True)\n",
    "print(f'Всего упоминалось: {len(sorted_by_number)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('виферон', 53),\n",
       " ('анаферон', 47),\n",
       " ('циклоферон', 41),\n",
       " ('глицин', 26),\n",
       " ('кагоцел', 25),\n",
       " ('ацикловир', 22),\n",
       " ('валериана', 22),\n",
       " ('ликопид', 22),\n",
       " ('гриппферон', 20),\n",
       " ('амизон', 17)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_number[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
